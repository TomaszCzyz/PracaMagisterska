%%% & --translate-file=cp1250pl
%% ************ AKADEMIA GÓRNICZO-HUTNICZA W KRAKOWIE **************
%% ***************** Wydział Matematyki Stosowanej ***************** 
%% ****************** PRACA MAGISTERSKA w LaTeX-u ******************
%%    autor: Tomasz Czyż
%%    Copyright (C) 2003 by ------
%% ************************* Plik główny *************************

%%
%% ======== PREAMBUŁA ======== 
%%
\documentclass[oik, pdftex, robocza, man]{mgrwms}

\usepackage[utf8]{inputenc}  % opcja latin2 dla Linux
\usepackage{amsmath}           % łatwiejszy skład matematyki
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\DeclareMathOperator*{\esssup}{ess\,sup}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{latexsym}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{mathtools}

\usepackage{color}

\usepackage[polish]{babel}
\usepackage[OT4]{fontenc}
\usepackage{polski}
\allowdisplaybreaks
%% <<<< BiBTeX >>>>
% \bibliographystyle{ddabbrv}
% \nocite{*}


\begin{document}
%%
%% ======== METRYCZKA PRACY ========
%%
\title{ \LARGE Aproksymacja funkcji kawałkami regularnych przy użyciu informacji dokładnej i niedokładnej}
\author{Tomasz Czyż}
\promotor{dr Maciej Goćwin}
\nralbumu{290565}
\maketitle

\slowakluczowe{słowa kluczowe}
\keywords{keywords}
%%
%% ======== MAKRA ========
%%
%-> Miejsce na nasze makra (jedno z wielu ;). Lepszym pomysłem będzie jednak 
%-> umieszczenie ich w osobnym pliku i wczytanie poleceniem \input
%%
\newtheorem{thm}{Twierdzenie}[chapter]
\newtheorem{lemma}[thm]{Lemat}
\newtheorem{stw}[thm]{Stwierdzenie}
\newtheorem{cor}[thm]{Wniosek}
\newtheorem{obs}[thm]{Obserwacja}
\newtheorem{uw}[thm]{Uwaga}
\newtheorem{df}[thm]{Definicja}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Pra}{\mathbb{Pra}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{G^{r,\varrho}([a,b])}
\newcommand{\1}{\mathbbm{1}}

\makeatletter
\newcommand*{\defeq}{\mathrel{\rlap{%
                     \raisebox{0.3ex}{$\m@th\cdot$}}%
                     \raisebox{-0.3ex}{$\m@th\cdot$}}%
                     =}
\let\c@table\c@figure
\makeatother

%%
%% ======== SPIS TREŚCI ========
%%
\tableofcontents
%%
%% ======== STRESZCZENIE PRACY (POLSKIE) ========
%%

\begin{streszczenie}
    Streszczenie
\end{streszczenie}

%%
%% ======== STRESZCZENIE PRACY (ANGIELSKIE) ========
%%

\begin{abstract}
    Abstract
\end{abstract}

%%
%%
%% ======== GŁÓWNA CZĘŚĆ PRACY ========
%%
%%

%%
%% ==== WSTĘP ====
%%

\begin{wstep}    % ew. \begin{wstep}[Wprowadzenie]
    Celem niniejszej pracy jest analiza zachowania różnych algorytmów aproksymujących funkcje kawałkami gładkie przy użyciu informacji dokładniej i niedokładniej.
    % Pierwszy z analizowanych algorytmów został przedstawiony w pracy \cite{PoA}. Rozważane są w niej funkcje klasy $F^{\infty}_r$, o których zakładamy, że zarówno sama funkcja $f: [0,T] \longrightarrow \R$ może być nieciągła, jak i jej pochodna, począwszy od rzędu możliwe większego od pierwszego. Czyli, dla przykładu, $f$ może być dwa razy różniczkowalna na $[0, T]$ i $f^{(3)}(s)$ może nie istnieć w jakimś punkcie $s$. Ponadto, $f$ może mieć skończenie wiele punktów osobliwych; ich ilość i położenie jest nieznane. Dodatkowo, algorytm przedstawiony w \cite{PoA} używa $n$ wartości funkcji w punktach $x_1, \ldots x_n$ jako jedyne dostępne informacje o funkcji $f$, a w przypadku algorytmu adaptacyjnego dopuszczamy, że wybór $x_j$ zależy od $f(x_1), \ldots, f(x_{j-1})$.
    % W wymienionej pracy do znalezienia optymalnego algorytmu nieadaptacyjnego i adaptacyjnego w najgorszym przypadku oraz w przypadku asymptotycznym do mierzenia błędu stosowana jest m.in. norma $L^p (1 \leq  p < \infty)$.

    % W artykule \cite{UA} rozszerzony jest wynik prac \cite{IaA} i \cite{PoA} poprzez skupienie się na klasie funkcji ciągłych globalnie r-regularnych z co najwyżej jednym punktem osobliwym. W podanej pracy przedstawiony jest algorytm nieadaptacyjny, który asymptotycznie poprawia ograniczenie błędu z \cite{AoP}.

    % Ostatni z analizowanych algorytmów pochodzi z pracy \cite{AoP} i jako jedyny z przedstawionych algorytmów uwzględnia zaburzenie danych. W artykule uogólnione zostają rezultaty z \cite{PoA} i \cite{UA} poprzez wprowadzanie informacji niedokładniej oraz założenie, że wykładnik Höldera $\varrho \in (0,1]$. (...) Z tego powodu w pracy \cite{AoP} przedstawiony został nowy algorytm do lokalizacji osobliwości. Co więcej dla $\varrho = 0$, co odpowiada informacji dokładniej, przedstawiony algorytm jest nawet prostszy niż te z \cite{PoA} i \cite{UA}.

\end{wstep}


%%
%% ==== ROZDZIAŁ ====
%%

\chapter{Definicje}

Dla liczby całkowitej $ r \geq 0$, $\varrho \in (0,1]$ oraz $a < b$, przez $H_{r, \varrho}(a,b)$ oznaczamy przestrzeń funkcji $g: [a,b] \rightarrow \R$ takich, że $g \in C^r([a, b])$ i $g^{(r)}$ jest Hölderowsko ciągła z wykładnikiem $\varrho$, tzn.
\begin{equation*}
    c(g) := \sup_{a \leq x \leq y \leq b} \frac{|g^{(r)}(x) - g^{(r)}(y)|}{|x-y|^{\varrho}} < \infty.
\end{equation*}
Dla danego $T > 0$ niech $F_{r, \varrho} = F_{r, \varrho}(T)$ będzie przestrzenią funkcji $f: \R \rightarrow \R$ spełniających następujące warunki: istnieje $s_f \in [0, T)$ i $g_f \in H_{r, \varrho}(0,T)$ takie, że
\begin{equation*}
    f(lT + s_f + x) = g_f(x) \quad \text{for all} \quad l = 0, \pm 1, \pm 2, \ldots \quad \text{i} \quad x \in [0, T)
\end{equation*}
Można powiedzieć, że $f$ jest 'kopią' $g_f$ na każdym z przedziałów $(lT + s_f, (l + 1)T + s_f]$ i $f$ jest prawostronnie ciągła na $lT + s_f$. W związku z tym wszystkie punkty, które różnią się między sobą o wielokrotność $T$ będą uważane za identyczne. Dla przykładu, jeżeli $0 < x_1 \leq T < x_2 \leq 2T$, to przedział $(x_1, x_2]$ będzie utożsamiany z $(x_1,T] \cup (0, x_2 - T] \subset (0, T]$.

Przez $\Delta_f^{(j)}$ ozanaczmy \emph{skoki nieciągłości} dla kolejnych pochdnych $f$ w punkcie nieciągłości $s_f$,
\begin{equation*}
    \Delta_f^{(j)} = f^{(j)}(s_f^+) - f^{(j)}(s_f^-) = g_f^{(j)}(0) - g_f^{(j)}(T) \quad 0 \leq j \leq r,
\end{equation*}

Z względu na sposób wyboru punktów $x_j$, $1 \leq j \leq n$, rozróżniamy \textit{informacje nieadaptacyjną}, gdy punkty wybierane są niezależnie od $f$ oraz \textit{informacje adaptacyjną}, gdy wybór $x_j$ zależy od poprzednio otrzymanych wartości $f$. W przypadku informacji adaptacyjnej $x_1$ jest ustalone natomiast
\begin{equation*}
    x_j = x_j(y_1, y_2, \ldots, y_{j-1}) \quad j \geq 2.
\end{equation*}

Przez \textit{informację} rozumiemy znane wartości aproksymowanej funkcji, Informację będziemy utożsamiać z wielowartościowym operatorem $N$. W przypadku, gdy $f \in F_{r, \varrho}$ przez $N(f)$ oznaczamy zbiór wszystkich możliwych informacji o funkcji $f$

\begin{equation*}
    y = (y_{1}, y_{2}, \dots, y_{n})
\end{equation*}

\noindent
Wtedy $N\!: F_{r, \varrho} \longrightarrow Y$, gdzie $Y$ jest \textit{zasięgiem} $N$, to znaczy $\displaystyle Y = \bigcup_{f \in F_{r, \varrho}} N(f)$

W tej pracy będziemy rozpatrywać aproksymacje funkcji $f \in F_{r, \varrho}$ względem normy $L^p$, gdzie $1 \leq p \leq \infty$.

Dla inforamcji $y$ oraz funkcji $f$ i jej aproksymacji $\phi(y)$, gdzie $\phi : Y \longmapsto L^p(0, T)$ błąd aproksymacji wynosi
\begin{equation*}
    \|f-\phi(y)\|_{L^p} = \left( \int_{0}^{T} |(f-\phi(y))(x)|^p \,dx  \right)^{1/p} \qquad if \; 1 \leq p < \infty
\end{equation*}
oraz
\begin{equation*}
    \|f-\phi(y)\|_{L^\infty} = \esssup_{0 < x \leq T} | (f - \phi(y))(x) |
\end{equation*}

\textit{Najgorszy przypadek błędu} definiujemy jako
\begin{equation*}
    e^{wor}_{p}(\phi, N, \mathcal{F}) = \sup_{f \in \mathcal{F}} \sup_{y \in N(f)} \|f - \phi(y) \|_{L^p}
\end{equation*}

Rozróżniamy następujące klasy $\mathcal{F}$:
\begin{equation*}
    \begin{split}
        \mathcal{K} & = \{f = c \1_{\R}, c \in \R \}, \\
        \mathcal{H}_{r, \varrho} & = \{ f \in F_{r, \varrho}: c(g_{f}) \leq 1, \Delta_{f}^{(j)} = 0 \; for\; all\; 0 \leq j \leq r\} \\
        \mathcal{F}_{r, \varrho}^{C} & = \{ f \in F_{r, \varrho}: c(g_{f}) \leq 1, \Delta_{f}^{(0)} = 0 \} \\
        \mathcal{F}_{r, \varrho}^{D} & = \{ f \in F_{r, \varrho}: c(g_{f}) \leq 1, | \Delta_{f}^{(0)} | \leq 1 \} \\
        \mathcal{F}_{r, \varrho} & = \{ f \in F_{r, \varrho}: c(g_{f}) \leq 1 \} \\
    \end{split}
\end{equation*}
Oczywiście
\begin{equation*}
    \mathcal{K} \subset \mathcal{H}_{r, \varrho} \subset \mathcal{F}_{r, \varrho}^{C} \subset \mathcal{F}_{r, \varrho}^{D} \subset \mathcal{F}_{r, \varrho}
\end{equation*}

Zauważmy, że zapis $y \in N(f)$ to formalny sposób powiedzenia, że $y$ jest informacją o $f$. Dla przykładu, dla informacji nieadaptacyjnej składającej się z zaburzonych ewaluacji funkcji $f$ w punktach $x_{1}, \dots, x_{n}$ z precyzją $\delta$, mamy:

\begin{equation*}
    N(f) = \{y \in \R^{n} : \quad |y_{i} - f(x_{i}| \leq \delta, \quad 1 \leq i \leq n\}
\end{equation*}

\noindent
Gdy $\delta = 0$, wtedy $N(f)$ jest singletonem a informacja jest dokładna.

Oznaczmy przez $\mathcal{N}(n, \delta)$ klasę wszystkich (adaptacyjnych) informacji $N$, które używają co najwyżej $n$ ewaluacji funkcji, z precyzją $\delta$ każda. Wtedy przez \textit{minimalany błąd najgorszego przykładu} w klasie $\mathcal{F}$, który może zostać osiągnięty przez algorytm używający informacji o co najwyżej $n$ wartościach funkcji z precyzją $\delta$ rozumiemy:

\begin{equation*}
    r^{wor}_{p}(n, \delta, \mathcal{F}) = \inf\{ e^{wor}_{p}(\varphi, N, \mathcal{F}) : \; \varphi \; uzywa \; N \in \mathcal{N}(n, \delta) \}
\end{equation*}

Poniższe ograniczenia dolne na $r^{wor}_{p}(n, \delta, \mathcal{F})$ są dość oczywiste albo mogą zostać wyprowadzone ze znanych rezultatów

\begin{stw}
\label{stw1}
    Dla każdego $n$ i $\delta \geq 0$ mamy:
    \begin{enumerate}[label=(\roman*)]
        \item \label{stw1_i} $r^{wor}_{p}(n, \delta, \mathcal{K}) \leq \delta T^{1/p}$
        \item \label{stw1_ii} $r^{wor}_{p}(n, \delta, \mathcal{H}_{r,\varrho}) \leq a_{r,\varrho}n^{-(r + \varrho)}$ dla pewnego $a_{r,\varrho} > 0$
        \item \label{stw1_iii} $r^{wor}_{p}(n, \delta, \mathcal{F}_{r,\varrho}) = \infty, \quad r \geq 1$
    \end{enumerate}
\end{stw}
\begin{proof}
    $(\dots)$
\end{proof}

Z stwierdzenia \ref{stw1} \textit{\ref{stw1_i}-\ref{stw1_ii}} otrzymujemy, że 
\begin{equation*}
    r^{wor}_{p}(n, \delta, \mathcal{F}^{D}_{r,\varrho}) \geq r^{wor}_{p}(n, \delta, \mathcal{F}^{C}_{r,\varrho}) \geq \max(\delta T^{1/p}, a_{r,\varrho} n^{-(r+\varrho)})
\end{equation*}
W dalszej części pracy udowodnimy, że te nierówności są ostre, z wyjątkiem pierwszej dla $p=\infty$. To jest główny wyniki artykułu \cite{AoP}, który możemy zapisać w poniższej postaci:

\begin{thm}
\label{AoP_tw1}~ %
    \begin{enumerate}
        \item $r^{wor}_{p}(n, \delta, \mathcal{F}^{D}_{r,\varrho}) = \Theta(\max(\delta, n^{-(r+\varrho)})) \quad dla \: 1 \leq p \leq \infty$
        \item $r^{wor}_{\infty}(n, \delta, \mathcal{F}^{C}_{r,\varrho}) = \Theta(\max(\delta, n^{-(r+\varrho)}))$
    \end{enumerate}
\end{thm}

W celu udowodnienia powyższego twierdzenia skonstruujemy algorytm, który posiada żądane własności błędu. W tym celu przedstawimy dodatkowe rezultaty dotyczące błędu interpolacji/ekstrapolacji funkcji kawałkami holderowskich, bazujących na wartościach zaburzonych.

\section{Rezultaty pomocnicze}

Niech $m \geq 2r + 1$, $h + \frac{T}{m}$ oraz $t_{i} = ih$ dla każdego $i$. Przez $d_{i}$ oznaczmy różnicę dzieloną stopnia $r+1$ bazującą na wartościach $f(t_{i})$:
\begin{equation*}
    d_{i} = f[t_{i}, \dots, t_{i+r+1}] = \sum_{j = 1}^{i+r+1} \gamma_{j} \prod_{k=1 \land k \neq j}^{i+r+1}(t_{k}-t_{j})^{-1}
\end{equation*}

Następnie oznaczmy przez $\tilde{d_i}$ (niedokładną) różnicę dzieloną stopnia $r+1$ bazującą na wartościach $y_{j} = F(t_{j}) + e_{j}$, gdzie $|e_{j}| \leq \delta$
\begin{equation*}
    \tilde{d_{i}} = \tilde{f}[t_{i}, \dots, t_{i+r+1}] = \sum_{j = 1}^{i+r+1} \gamma_{j} \prod_{\substack{k=1 \\ k \neq j}}^{i+r+1}(t_{k}-t_{j})^{-1}
\end{equation*}

\begin{lemma}
    Jeżeli $f \in H_{r, \varrho}(t_{i}, t_{i+r+1})$, wtedy
    \begin{equation*}
        |\tilde{d_{i}}| \leq \frac{c(g_{f})(r+1)^{\varrho}}{(r+1)!} h^{\varrho-1} + \delta \frac{2^{r+1}}{(r+1)!} h^{-(r+1)}
    \end{equation*}
\end{lemma}
\begin{proof}
    $(\dots)$
\end{proof}

Teraz oszacujemy błąd interpolacji i ekstrapolacji w obecności zaburzenia wartości funkcji. Niech $p_{i}$ i $\tilde{p_{i}}$ odpowiadają wielomianom stopnia co najwyżej $r$ interpolujących $f$ opartych na dokładnych i niedokładnych wartościach funkcji $f$ w punktach $t_{i}, t_{i+1}, \dots, t_{i+r}$. Dla $r \geq 1$, wprowadźmy oznaczenia:
\begin{equation*}
    \beta_{r} = \max_{0 \leq t \leq r} |\prod_{k=0}^{r} (t-k)|, \quad
    \Lambda_{r} = \max_{0 \leq t \leq r} \sum_{k=0}^{r} \prod_{\substack{l=0 \\ l \neq k}}^{r} \left| \frac{t-l}{k-l} \right| \quad
    \tilde{\Lambda}_{r} = \sum_{k=0}^{r} \prod_{\substack{l=0 \\ l \neq k}}^{r} \left| \frac{2r+1-l}{k-l} \right|
\end{equation*}

\begin{lemma}
    Niech $f \in H_{0, \varrho}$, wtedy: \\
    $dla \; x \in [t_{i-\frac{1}{2}}, t_{i + \frac{1}{2}}] :$
    \begin{equation*}
        |f(x) - \tilde{p}_{1}(x)| \leq C_{0, \varrho}(f) h^{\varrho} + \delta, \quad C_{0, \varrho}(f) = c(g_{f}) 2^{-\varrho} \\
    \end{equation*}
    $dla \; x \in [t_{i-1}, t_{i - \frac{1}{2}}) \cup (t_{i + \frac{1}{2}}, t_{i+1}]  :$
    \begin{equation*}
        |f(x) - \tilde{p}_{i}(x)| \leq \overline{C} _{0, \varrho}(f) h^{\varrho}  + \delta, \quad \overline{C} _{0, \varrho}(f) = c(g_{f}) \\
    \end{equation*}
    Niech $f \in H_{r, \varrho}$ i $r \geq 1$, wtedy: \\
    $dla \; x \in [t_{i}, t_{i + r}] : $
    \begin{equation*}
        |f(x) - \tilde{p}_{i}(x)| \leq C_{r, \varrho}(f) h^{r+\varrho} + \delta\Lambda_{r}, \quad C_{r, \varrho}(f) = c(g_{f}) 2^{-\varrho} \\
    \end{equation*}
    $dla \; x \in [t_{i-r-1}, t_{i}) \cup (t_{i + r}, t_{i+2r+1}]  :$
    \begin{equation*}
        |f(x) - \tilde{p}_{i}(x)| \leq \overline{C} _{r, \varrho}(f) h^{r+\varrho} + \delta\overline{\Lambda}_{r}, \overline{C} _{r, \varrho}(f) = c(g_{f}) \frac{(2r+1)!(2r+1)^{\varrho}}{r(r!)^{2}} \\
    \end{equation*}
\end{lemma}

\begin{proof}
    $(\dots)$
\end{proof}

\begin{lemma}
    Niech $f \in F_{r, \varrho}$ oraz 
    \begin{equation*}
        s_{f} \in 
        \left\{
            \begin{array}{ll}
                (t_{i-\frac{1}{2}}, t_{i + \frac{1}{2}}], \; gdy \; r=0 \\
                (t_{i}, t_{i - r}], \; gdy \; r \geq 0    
            \end{array}
        \right.
    \end{equation*}
    Przypuśćmy, że $|\tilde{d}_{k}| \leq Bh^{\varrho-1} \forall_{k}$. Wtedy dla każdego $x \in [t_{i-1}, t_{i+1}]$, gdy $r=0$ lub dla każdego $x \in [t_{i-r-1}, t_{i+2r+1}]$, gdy $r \geq 1$, mamy:
    \begin{equation*}
        |f(x) - \tilde{p}_{i}(x)| \leq D_{r}(B, f)h^{r+\varrho} + \delta\Lambda_{r},
    \end{equation*}
    gdzie $D_{0}(B,f) = c(g_{f}) + B$ i
    \begin{equation*}
        D_{r}(B, f)=c\left(g_{f}\right) \frac{\beta_{r}(r+1)^{\varrho}}{r r !}+B\left(2^{r+1}-1\right) \frac{(2 r) !}{(r-1) !} \quad \text { for } r \geq 1 .
    \end{equation*}
\end{lemma}
\begin{proof}
    $(\dots)$
\end{proof}

\subsection{(Podstawowe definicje z artykułów \cite{PoA} i \cite{UA})}

Dla $r \geq 1$, $a < b$, przez $W_{r}(a,b)$ oznaczamy przestrzeń funkcji $r$-gładkich, zdefiniowanych w następujący sposób:
\begin{equation*}
    W_{r}(a, b)=\left\{f \in C^{r-1}([a, b]) \mid \quad f^{(r-1)} \text{ absolutnie ciągła, }\left\|f^{(r)}\right\|_{L^{\infty}(a, b)}<\infty\right\}
\end{equation*}

Przez $F_{r} = F_{r}(0, T)$, dla $T > 0$, oznaczamy funkcje $r$-gładkie, ale posiadające jeden punkt osobliwy, tzn. takich funkcji $f$, że $f \in W_{r}(0,T)$ albo
\begin{equation*}
    f(x)= \begin{cases}f_{-}(x), & 0 \leq x<s_{f} \\ f_{+}(x), & s_{f} \leq x \leq T\end{cases},
\end{equation*}
gdzie $f_{-}(x) \in W_{r}(0, s_{f})$, $f_{+}(x) \in W_{r}(s_{f}, T)$, dla $s_{f} \in (0,T)$. Innumi słowy $f \in F_{r}$ wtw, gdy
\begin{equation*}
    f(x)=g(x)+\1_{\left[s_{f}, T\right]}(x) \sum_{j=0}^{r-1} \Delta_{f}^{(j)} \frac{\left(x-s_{f}\right)^{j}}{j !}, \quad 0 \leq x \leq T
\end{equation*}
gdzie $g \in W_{r}(0,T)$ i $\Delta_{f}^{(j)}$ to skoki nieciągłości odpowiadającej pochodnej w punkcie $s_{f}$. Warto zauważyć, że 
$0 \leq a<b \leq T$ mamy $\left\|f^{(r)}\right\|_{L^{\infty}(a, b)}=\left\|g^{(r)}\right\|_{L^{\infty}(a, b)}$

W pracy \cite{UA} skupiamy się na klasie funkcji $G_{r} = G(0,T) \coloneqq F_{r}(0,T) \cap C([0,T])$. Stąd wynika, że $f \in G$ wtw, gdy $f \in F_{r} \land \Delta_{f}^{(0)} = 0$.
Zakładamy też, że $r \geq 2$.






\mgrclosechapter

%%
%% ==== ROZDZIAŁ ====
%%

\chapter{Złożoność obliczeniowa}

% notacja O(n) itp...


\mgrclosechapter


%%
%% ==== ROZDZIAŁ ====
%%

\chapter{Informacja dokładna i niedokładna}

\noindent
rozdział o informacji zaburzonej itp.

Przez informacje zaburzoną rozmumiemy
\begin{equation*}
    y_j = f(x_j) + e_j, \quad 1 \leq j \leq n
\end{equation*}
gdzie $n$ to liczba odwołań algorytmu do funkcji $f$


\mgrclosechapter



%%
%% ==== ROZDZIAŁ ====
%%

\chapter{Ograniczenia z dołu}


\mgrclosechapter


%%
%% ==== ROZDZIAŁ ====
%%

\chapter{Algorytmy}

\section{Algorytm oparty o informację dokładną}

Algurytm przedstawiony w pracy \cite{CoDF} lokalizuje osobliwość przy pomocy wielomianów Lagrange'a $w_{g}^{r}$. Na wejściu algorytm otrzymuje $g \in \G$, przedział $[a,b]$, regularność $r$ oraz współczynnik Höldera $\varrho$. Kluczowym elementem algorytmu jest zdefiniowana poniżej wielkość (\textit{test}), która jest użyta do wykrycie punktu osobliwego.
\begin{equation}
    \label{eqn:test}
    A_{g}(a, \bar{a}, \bar{b}, b)=\max _{0 \leq j \leq r} \frac{\left\|w_{g}^{r}([\bar{b}, b])\left(z_{j}\right)-w_{g}^{r}([a, \bar{a}])\left(z_{j}\right)\right\|}{\bar{h}^{r+e}},
\end{equation}
gdzie $a<\bar{a}<\bar{b}<b$, $z_{j} = \bar{a} + (\bar{b} - \bar{a})j/r$, dla $j=0,1,\dots,r$ oraz $\bar{h} = b - a$ jest długością przedziału, na którym \textit{test} jest zdefiniowany.

\vspace{10pt}
\begin{tabular}{p{0.045\linewidth} p{0.85\linewidth}}
    \textit{K1:}    & Niech $\delta \coloneqq h^{r+\varrho}$, $B \coloneqq \emptyset$ \\
                    & \textbf{jeżeli} \(\displaystyle \max_{0 \leq i \leq p-1} (c_{i+1} - c_{i}) \leq 4\delta \) \textbf{wtedy} \\
                    & $\quad$ idź do \textit{Krok 3} \\
                    & \textbf{w p.p.} \\
                    & $\quad$ Niech, dla $j=0,1, \ldots, p-1$, \\
                    & $\quad$ $A=\max \left\{A_{g}\left(c_{j}, c_{j}+\delta, c_{j+1}-\delta, c_{j+1}\right) \mid c_{j+1}-c_{j}>4 \delta\right\}$ \\
                    & $\quad$ Niech $A_{g}^{i} = A_{g}\left(c_{i}, c_{i}+\delta, c_{i+1}-\delta, c_{i+1}\right)$ \\
                    & $\quad$ \textbf{jeżeli} istnieją różne $k$ i $l$ takie, że $A = A^{k} \land A = A^{l}$ \textbf{wtedy} \\
                    & $\quad\quad$ idź do \textit{Krok 3} \\
                    & \\

    \textit{K2:}    & Niech $[c_{k}, c_{k+1}]$ - przedział otrzymany w \textit{Krok 1}\\
                    & $B = BISECTION(g, [a,b], r, \varrho)$\\
                    & \\

    \textit{K3:}    & Niech $\bar{M} = \left\{ c_{0}, \dots, c_{p} \right\} \cap B$ \\
                    & $q(t)= \begin{cases}
                        g\left(c_{i}\right)                                                 &\text{gdy } t \in \left[c_{i}, c_{i+1}\right) \land c_{i+1}-c_{i} \leq 4 \delta \\ 
                        g\left(c_{i}\right)                                                 &\text{gdy } t \in \left[c_{i}, c_{i}+\delta\right) \land c_{i+1}-c_{i}>4 \delta, \\ 
                        w_{g}^{r}\left(\left[c_{i}+\delta, c_{i+1}-\delta\right]\right)(t)  &\text{gdy } t \in\left[c_{i}+\delta, c_{i+1}-\delta\right) \land c_{i+1}-c_{i}>4 \delta \\ 
                        g\left(c_{i+1}-\delta\right)                                        &\text{gdy } t \in\left[c_{i+1}-\delta, c_{i+1}\right) \land c_{i+1}-c_{i}>4 \delta
                        \end{cases}$ \\
                    & dla $i=0,1,\dots,k-1$ z $q(b) = $ zdefiniowanym przez ciągłość na ostatnim przedziale \\
                    & $B \coloneqq B \cup \left\{ c_{i} + \delta, c_{i+1} - \delta \mid c_{i+1} - c_{i} > 4\delta,\; i=0,\dots,k-1 \right\}$ \\

\end{tabular} \vspace{10pt}

\subsection{Analiza algorytmu}

\begin{thm}
    \label{2014_tw1}
    Niech $r+\varrho \geq 1$. Istnieją stałe $C$ i $m_{0}$ takie, że dla $m>m_{0}$, przedziału $[a,b]$ oraz wszystkich $g \in \G$ z $\delta_{g}^{0} = 0$ zachodzi:
    \begin{equation*}
        \sup_{t \in [a,b]} \| g(t) - q(t) \| \leq Cm^{-(r+\varrho)}
    \end{equation*}
    Dodatkowo, obliczenie $q$ wymaga $O(p+\log m)$ ewaluacji fun $g$, gdzie $p$ jest liczbą przedziałów w początkowym podziale $M$ przedziału $[a,b]$. Czyli, aby otrzymać optymalną aproksymację $g \in \G$ na przedziale $[a,b]$, wystarcza wiąźć podział $M$ z $m+1$ równoodległymi punktami $x_{i} = a+(b-a)i/m$, dla $i=0,1,\dots,m$. wtedy obliczenie $q$ wymaga $O(m)$ ewaluacji funkcji $g$.
\end{thm}

Zacznijmy od wyjaśnienia własności testu \ref{eqn:test} służącego do wykrywania osobliwości. Rozważmy błąd interpolacji Lagrange'a dla nieciągłej funkcji $g \in \G$. Błąd jest ograniczony za względu na wielomian $s^{g}$ (TODO: reference).

\begin{lemma}
    Istnieje stała $C$ taka, że dla wszystkich $[a,b]$, wszystkich $g \in \G$ oraz $s=0,1,\dots,r$, mamy
    \begin{equation*}
        \sup _{t \in[a, b]}\left\|g(t)-w_{g}^{s}([a, b])(t)\right\| \leq C\left(\min \left\{\sup_{t \in[a, \hat{t}_{g})}\left\|s_{g}(t)\right\|, \sup _{t \in [\hat{t}_{g}, b]}\left\|s_{g}(t)\right\|\right\}+\bar{h}^{\min \{s+1, r+\varrho\}}\right)
    \end{equation*}
\end{lemma}
\begin{proof}
    $(\dots)$
\end{proof}

\begin{cor}
    Istnieje stała $C$ taka, że dla wszystkich $[a,b]$, wszystkich $g \in \G$ z $\delta_{g}^{0} = 0$, $0 \leq \delta \leq \min{1, \bar{h}}$ oraz $s=0,1,\dots,r$, mamy
    \begin{equation*}
        \hat{t}_{g} \in(a, a+\delta] \cup[b-\delta, b) \Longrightarrow  \sup_{t \in[a, b]}\left\|g(t)-w_{g}^{s}([a, b])(t)\right\| \leq C\left(\delta+\bar{h}^{\min \{s+1, r+\varrho\}}\right)
    \end{equation*}
\end{cor}
\begin{proof}
    $(\dots)$
\end{proof}

\begin{lemma}
    Istnieje stała $C$ zależna od $r$ i $L_{r}$ taka, że dla wszystkich $[a,b]$, $\bar{a} \in (a,b)$, wszystkich $g \in \G$, mamy
    \begin{equation*}
        \hat{t}_{g} \in(\bar{a}, b) \Longrightarrow g(t)-w_{g}^{r}([a, \bar{a}])(t)=s_{g}(t) \1_{\left[\hat{t}_{g}, b\right]}(t)+R_{g}(t), \quad t \in[\bar{a}, b],
    \end{equation*}
    gdzie $\| R_{g}(t) \| \leq C\bar{h}^{r+\varrho}$, dla $t \in [\bar{a}, b]$
\end{lemma}
\begin{proof}
    $(\dots)$
\end{proof}

\begin{cor}
    Istnieje stała $\bar{C}$ zależna od $r$ i $L_{r}$ taka, że dla wszystkich $[a,b]$, $\bar{a} \in (a,b)$, wszystkich $g \in \G$, mamy
    \begin{equation*}
        \hat{t}_{g} \in (a,\bar{a}) \Longrightarrow g(t)-w_{g}^{r}([\bar{a},a])(t)=s_{g}(t) \1_{\left[a,\hat{t}_{g}\right]}(t)+R_{g}(t), \quad t \in[a,\bar{a}],
    \end{equation*}
    gdzie $\| R_{g}(t) \| \leq \bar{C}\bar{h}^{r+\varrho}$, dla $t \in [a,\bar{a}]$
\end{cor}
\begin{proof}
    $(\dots)$
\end{proof}

\begin{stw}
    \label{2014_stw1}
    Istnieje stała $C^{*}$ zależna od $r$ i $L_{r}$ taka, że dla wszystkich $a < \bar{a} < \bar{b} < b$ i $[a,b]$ oraz wszystich $g \in \G$, mamy
    \begin{equation*}
        \hat{t}_{g} \text{ z niezerowym wielomianem } s_{g} \text{ nie jest w } (a,b) \Longrightarrow A_{g}(a, \bar{a}, \bar{b}, b) \leq C^{*}
    \end{equation*}
\end{stw}
\begin{proof}
    $(\dots)$
\end{proof}

\begin{uw}
    Stwierdzenie \ref{2014_stw1} pokazuje, że procedura LOCATE-APPROXIMATE wraz z zprocedurą BISECTION, sukcesywnie wybiera przedziały bazując a wartościach testu. Zauważmy, że jeżeli $\hat{t}_{g}$ jest unikalna, to wtedy dla jakiegokolwiek przedziału $[a,b]$, który nie został wybrany, mamy $A_{g}(a, \bar{a}, \bar{b}, b) \leq C^{*}$
\end{uw}

\begin{stw}
    \label{2014_stw2}
    Niech $D > 0$. Istnieją stałe $C$ i $\bar{N}$, zależne tylko od parametrów klast $\G$ i $D$, takie, że dla wszystkich $[a,b]$, $[\bar{a}, \bar{b}] \subset (a,b)$, $g \in \G$ oraz $s=0,1,\dots,r$, mamy
    \begin{equation*}
        \hat{t}_{g} \in (\bar{a}, \bar{b}] \land b-a \leq D(\bar{b}-\bar{a}) \Longrightarrow \text{dla } [\gamma, \omega]=[a, b] \vee [\gamma, \omega]=[\bar{a}, \bar{b}] \text{ zachodzi }
    \end{equation*}
    \begin{equation*}
        \sup _{t \in[\gamma, \omega]}\left\|g(t)-w_{g}^{s}([\gamma, \omega])(t)\right\| \leq C\left(1+A_{g}(a, \bar{a}, \bar{b}, b)\right) \bar{h}^{\min \{s+1, r+\varrho\}}
    \end{equation*}
    oraz ponadto
    \begin{equation*}
        \sup _{t \in[\gamma, \omega]}\left\|\left(w_{g}^{s}([\gamma, \omega])\right)^{(j)}(t)\right\| \leq \bar{N}\left(1+\bar{h}^{\min \{s+1-j, r+\rho-j\}}+\left(1+A_{g}(a, \bar{a}, \bar{b}, b)\right) \bar{h}^{r+\varphi-j}\right)
    \end{equation*}
    dla $j=0,1,\dots,s$.
\end{stw}
\begin{proof}
    $(\dots)$
\end{proof}

\begin{uw}
    Stwierdzenie \ref{2014_stw2} pokazuje, że w przypadku z osobliwością, ograniczenie górne na błąd interpolacji możemy wyrazić za pomocą $A_{g}(a, \bar{a}, \bar{b}, b)$
\end{uw}

Poniższy lemat dotyczy przypadku, gdy osobliwość znajduje się na brzegu przdziału $[a,b]$\\
\textit{(lemat... dowod...)}

\textit{(Dowód tw1 z 2014)}

\begin{uw}
    Twierdzenie \ref{2014_tw1} zachodzi również dla funkcji $g$, która ma skok w punkcie $c_{i}$ początkowego podziału $M$ oraz ma niezerowy wielomian $s_{g}$ dla co najwyżej jednego nieznanego punktu $t_{g}$, $t_{g} \neq c_{i} \; \forall_{i}$.
\end{uw}


\section{Algorytm oparty o informację zaburzoną}

W tym rozdziale opiszemy algorytm bazujący na informacji zaburzonej przedstawiony w artykule \cite{AoP}. Analizowany algorytm używa co najwyżej $n$ wartości funkcji z precyzją $\delta $ oraz w najgorszym przypadku ma błąd proporcjonalny do $\max{(\delta, n^{-1 / r + \varrho })}$ w klasie funkcji $\F^D_{r,\varrho }$ dla $p < \infty$ oraz w klasie $\F^C_{r,\varrho }$ dla $p \leq \infty$. Kluczowym parametrem algorytmu jest
$$
    h = T / m \quad with \quad  m \geq 2r + 1,
$$
gdzie $m$ jest początkową gęstością siatki. Dodatkowo, niech $\omega  = \omega(h)$ spełnia $0 < \omega < (r + 1)h $.

Na początku algorytm aproksymuje punkt osobliwy $s_f$. Jest to realizowane w trzech krokach. W kroku 1. przy pomocy siatki rozmiarze o długości $h$ i różnic dzielonych lokalizowany jest punkt $s_f$ na przedziale $[u_1, v_1]$ o długości $(r + 1)h$. W kroku 2. używamy wielomianów interpolujących $\tilde{p}_+$ i $\tilde{p}_-$ do zwężenia tego przedziału do $[u_2, v_2]$. Krok 3. produkuje przedział $[u_3, v_3] \subseteq [u_2, v_2]$, w którym różnica $|\tilde{p}_{+} - \tilde{p}_{-}|$ jest nierosnąca na $[u_3, \xi]$ i niemalejąca na $[\xi, v_3]$, gdzie $\xi$ jest finalną aproksymacją $s_f$.

Powyższe kroki mogą być zapisane następująco; dla $t_i = ih \; \forall i$. \vspace{10pt}

\noindent
\begin{tabular}{p{0.10\linewidth} p{0.85\linewidth}}
    
    \textit{Krok 1} & Oblicz różnice dzielone $\tilde{d}_i = \tilde{f}[t_i, \ldots, t_{i+r+1}]$ for $1 \leq i \leq m $ oraz znajdź \\
                    & \(\displaystyle \qquad i^* = arg \max_{1 \leq i \leq m }|\tilde{d}_i| \)  \\
                    & Niech $u_1 = t_{i^*}$ i $v_1 = t_{i^* + r + 1}$. \\
                    & \\

    \textit{Krok 2} & Oznaczymy przez $\tilde{p}_+$ i $\tilde{p}_-$ wielomiany stopnia $ \leq r$, które interpolują węzły $(t_j, \tilde{f}(t_j))$ odpowiednio dla $i^* - r \leq j \leq i^*$ oraz dla $i^* + r + 1 \leq j \leq i^* + 2r + 1$. Następnie wykonaj iterację: \\
                    & $u := u_1$, $v := v_1$ \\
                    & \textbf{dopóki} $v-u > \omega$ \textbf{wykonuj}: \\
                    & $\quad$$z_j := u + j(v-u) / (r+2), \qquad j = 1, 2, \ldots, r + 1$ \\
                    & $\quad$\(\displaystyle j^* := arg \max_{1 \leq j \leq r + 1}|\tilde{p}_{+}(z_j) - \tilde{p}_{-}(z_j)| \) \\
                    & $\quad$\textbf{jeżeli} $|\tilde{f}(z_{j^*}) - \tilde{p}_{-}(z_j)| \leq |\tilde{f}(z_{j^*}) - \tilde{p}_{+}(z_j)|$ \textbf{wtedy} \\
                    & $\quad\quad$$u:= z_{j^*}$ \\
                    & $\quad$\textbf{w p.p.} \\
                    & $\quad\quad$$v:= z_{j^*}$ \\
                    & \textbf{koniec} \\
                    & Niech $u_2 = u$ i $v_2 = v$. \\
                    & \\

    \textit{Krok 3} & Wykonaj iterację: \\
                    & $u := u_2$, $v := v_2$ \\
                    & \textbf{dopóki} istnieje maksimum lokalne $|\tilde{p}_{+} - \tilde{p}_{-}|$ na $(u,v)$ \textbf{wykonuj} \\
                    & $\quad$$z :=$ największe maksimum lokalne $|\tilde{p}_{+} - \tilde{p}_{-}|$ na $(u,v)$ \\
                    & $\quad$\textbf{jeżeli} $|\tilde{f}(z) - \tilde{p}_{-}(z)| \leq |\tilde{f}(z) - \tilde{p}_{+}(z)|$ \textbf{wtedy} \\
                    & $\quad\quad$$u:= z$ \\
                    & $\quad$\textbf{w p.p.} \\
                    & $\quad\quad$$v:= z$ \\
                    & \textbf{koniec} \\
                    & Niech $u_3 = u$ i $v_3 = v$.
\end{tabular} \vspace{10pt}

Finalną aproksymacją $s_f$ jest
\begin{equation*}
        \xi := arg \max_{u_3 \leq x \leq v_3}|\tilde{p}_{+} - \tilde{p}_{-}| \hspace{200pt}
\end{equation*}

\subsection{Analiza algorytmu}

Przedstawiony algorytm używa $m$ wartości funkcji w kroku 1 oraz jedyną wartość funkcji w każdej iteracji w krokach 2 i 3. Czyli w kroku 2 używamy co najwyżej
\begin{equation*}
    \left\lceil\frac{\ln \left(\frac{(r+1) h}{\omega(h)}\right)}{\ln \left(\frac{r+2}{r+1}\right)}\right\rceil
\end{equation*}
wartości funkcji i $(r-1)$ w kroku 3.
Stąd otrzymujemy, że jeżezli $\omega = \omega(h) \geq kh^{\alpha}$ dla pewngo ustalonego $k$ i $\alpha$, wtedy w najgoryszym przypadku liczba użytych wartości funkcji równa sie asymptotycznie $m = \frac{T}{h}$ dla $h \rightarrow 0^{+}$. \\


\textit{(punktowa analiza błędu...)}


Podsumowując analizę błędu dla każdego punktu otrzymujemy, że gdy $\delta \leq bh^{r+\varrho}$ wtedy:
\begin{equation*}
    \begin{cases}
        |f(x) - \phi_{h}^{*}(y_{h})(x)| \propto \max(1, c(g_{f})) h^{r+\varrho} & \text{ dla } x \notin (u_{2}, v_{2}] \\
        |f(x) - \phi_{h}^{*}(y_{h})(x)| \propto \max(1, c(g_{f})) h^{r+\varrho} + |\Delta_{f}^{(0)}| & \text{ dla } x \in (u_{2}, v_{2}] \\
    \end{cases}
\end{equation*}
gdzie $v_{2} - u_{2} \leq \omega$

Mamy również, że liczba ewaluacji funkcji $n$ jest proporcjonalna do $h^{-1}$, tak więc $h^{r+\varrho}$ jest proporcjonalne do $n^{-(r+\varrho)}$. Z tego wynika poniższe stwierdzenia:
\begin{stw}
    \label{stw2}
    Niech $1 \leq p \leq \infty$. Jeżeli $\delta \leq bh^{r+\varrho}$ oraz $\omega(h) = h^{(r+\varrho)p + 1}$, wtedy
    \begin{equation*}
        e_{\mathrm{p}}^{wor}\left(\varphi_{h}^{*}, N_{h}^{*} ; \mathcal{F}_{r, \varrho}^{D}\right)=\mathcal{O}\left(n^{-(r+\varrho)}\right)
    \end{equation*}
\end{stw}

Warto wspomnieć, że powyższe ograniczenia górne nie może zostać spełnione przez algorytmy nieadaptacyjne, co zostało pokazane w \cite{PoA}. Pokazano tam również, że dla $p=\infty$ nie istnieje algorytm z błędem zbiegającym do zera, dlatego założenia $p < \infty$ jest niezbędne. Dodatkowo, gdy rozważymy klasę $\mathcal{F}_{r, \varrho}^{C} \subset \mathcal{F}_{r, \varrho}^{D}$, to możemy uprościć algorytm biorąc $\omega(h) = (r+1)h$ i unikając iteracji w kroku 2. Otrzymujemy w ten sposób algorytm, który dla $r=0,1$ jest nieadaptacyjny, a dla $r \geq 2$ używa co najwyżej $r-1$ dodatkowych punktów, niezależnie od tego jak małe jest $h$. Co więcej, organiczenie górne zachodzi dla $p = \infty$
Stosując powyższą modyfikację możemy sformułować następujące stwierdzenie.

\begin{stw}
    \label{stw3}
    Jeżeli $\delta \leq bh^{r+\varrho}$ i $\omega(h) = (r+1)h$, wtedy:
    \begin{equation}
        \mathrm{e}_{\infty}^{\mathrm{wor}}\left(\varphi_{h}^{*}, N_{h}^{*} ; \mathcal{F}_{r, \varrho}^{C}\right)=\mathcal{O}\left(n^{-(r+\varrho)}\right) .
    \end{equation}
\end{stw}

Ponownie, dla $r \geq 2$ użycie informacji adaptacyjnej jest konieczne. Łącząc wyniki \ref{stw2}, \ref{stw3} i \ref{stw1}\ref{stw1_i} otrzymujemy \ref{AoP_tw1}.
Faktycznie, dla ustalonego $\delta$ i $n$ możemy wybrać $h = \frac{T}{m}$ takie, że
\begin{equation}
    m = m(n, \delta)=\left\lfloor\min \left(\beta n, \frac{1}{T}\left(\frac{b}{\delta}\right)^{\frac{1}{r+\varrho}}\right)\right\rfloor=\varTheta\left(\min \left(n, \delta^{-1 /(r+\varrho)}\right)\right),
\end{equation}

\begin{uw}
    Zauważmy, że dla ustalonej precyzji $\delta$ nie ma sensu brać $m$ większego niż $m_{max} = \varTheta(\delta^{-1 / (r+\varrho)})$ wartości funkcji, ponieważ dla $m = m_{max}$ osiągamy maksymalną dokładność dla danego $\delta$.
\end{uw}




\mgrclosechapter


%%
%% ==== ROZDZIAŁ ====
%%

\chapter{Testy numeryczne}

porównanie algorytmów

\mgrclosechapter



%%
%% ======== DODATKI ========
%%
% \appendix
% \chapter{----}
%%
%-> Treść dodatku A
%%
% \mgrclosechapter

%%
%% ======== BIBLIOGRAFIA ========
%%

%% <<<< BiBTeX >>>>
% \bibliography{<pliki bib>} 
%%
\begin{thebibliography}{88}

    \bibitem{IaA}
    F. Arandiga, A. Cohen, R. Donat, N. Dyn,
    \emph{Interpolation and approximation of piecewise smooth functions}, SIAM J. Numer. Anal. 43 (2005) 41–57

    \bibitem{PoA}
    L. Plaskota, G. W. Wasilkowski, Y. Zhao, 
    \emph{The power of adaption for approximating functions with singularities}, Mathematics Of Computation 77
    2008, p. 2309–2338

    \bibitem{UA}
    L. Plaskota, G. W. Wasilkowski, 
    \emph{Uniform approximation of piecewise r-smooth and globally continuous functions}, SIAM Journal on Numerical
    Analysis, Vol. 47, No. 1 (2008/2009)

    \bibitem{CoDF}
    B. Kacewicz, P. Przybyłowicz, 
    \emph{Complexity of the derivative-free solution of
    systems of IVPs with unknown singularity hypersurface}, Journal of Complexity
    
    \bibitem{AoP}
    P. M. Morkisz, L. Plaskota, 
    \emph{Approximation of piecewise Hölder functions from inexact information}, Journal of Complexity

\end{thebibliography}

\end{document}
